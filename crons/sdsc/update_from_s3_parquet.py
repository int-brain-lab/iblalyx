import json
from pathlib import PurePosixPath, Path
import warnings

import pandas as pd
from iblutil.util import flatten
from one.alf import spec, io as alfio, files as alfiles

import boto3
from data.models import DataRepository, Dataset, FileRecord

LAB = 'churchlandlab_ucla'
ROOT = r'F:\\'


def get_s3_access(repo_name='aws_cortexlab'):
    """Return API access keys from Alyx data repository JSON field

    Parameters
    ----------
    repo_name : str
        The name of the data repository to access, should begin with 'aws'

    Returns
    -------
    bucket_name : str
        The name of the S3 bucket
    session_keys : dict
        The access key credentials to be passed to boto3.Session
    """
    repo_json = DataRepository.objects.get(name=repo_name).json
    bucket_name = repo_json['bucket_name']
    session_keys = {'aws_access_key_id': repo_json.get('Access key ID', None),
        'aws_secret_access_key': repo_json.get('Secret access key', None)}
    return bucket_name, session_keys


def get_s3_access_one(repo_name='aws_cortexlab', alyx_client=None):
    """Return API access keys via ONE from Alyx data repository JSON field

    Parameters
    ----------
    repo_name : str
        The name of the data repository to access, should begin with 'aws'
    alyx_client : one.webclient.AlyxClient
        An instance of the ONE AlyxClient to use

    Returns
    -------
    bucket_name : str
        The name of the S3 bucket
    session_keys : dict
        The access key credentials to be passed to boto3.Session
    """
    if not alyx_client:
        from one.webclient import AlyxClient
        alyx_client = AlyxClient()
    repo_json = alyx_client.rest('data-repository', 'read', id=repo_name)['json']
    bucket_name = repo_json['bucket_name']
    session_keys = {'aws_access_key_id': repo_json.get('Access key ID', None),
        'aws_secret_access_key': repo_json.get('Secret access key', None)}
    return bucket_name, session_keys


def get_bucket_list_key(bucket, location):
    """Return location of latest bucket list generated by Amazon Inventory.

    Parameters
    ----------
    bucket : boto3.resources.factory.s3.Bucket
        An S3 bucket instance that stores the bucket list
    location : str
        The location of the bucket list

    Returns
    -------
    inventory : dict
        A record of the most recent inventory file(s)

    See Also
    --------
    https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html

    """
    # Ensure trailing slash
    if location[-1] != '/':
        location += '/'
    # Find meta files
    meta = filter(lambda x: x.key.endswith('json'), bucket.objects.filter(Prefix=prefix))
    meta_latest = sorted(meta, key=lambda x: x.key, reverse=True)[0]
    return json.loads(meta_latest.get()['Body'].read())


# Instantiate S3 resource
bucket_name, session_keys = get_s3_access()
session = boto3.Session(**session_keys)
s3 = session.resource('s3')
bucket = s3.Bucket(bucket_name)

# Location of S3 inventory
prefix = '/'.join(['info', bucket_name, 'bucket-list'])
# Get latest inventory list file path
inventory = get_bucket_list_key(bucket, prefix)
format = inventory['fileFormat']
assert format == 'Parquet', f'inventory format "{format}" unsupported; check settings'
assert inventory['sourceBucket'] == bucket_name
# Currently multi-part inventory files are not supported; this may change as bucket grows
assert len(inventory['files']) == 1, 'multi-part inventory files not supported'
assert inventory['files'][0]['size'] < 1e9, 'inventory file too big!'
source = inventory['files'][0]['key']
destination = str(Path(ROOT).joinpath(f'{inventory["version"]}_{source.split("/")[-1]}'))

# Locate most recent inventory
bucket.download_file(source, destination)
df = pd.read_parquet(destination)

# Remove folders from dataframe
df = df[~df['key'].str.endswith('/')].copy()  # Copy avoids slice assign warnings below

# Prepare dataframe for some database information
df['isALF'] = True  # Filename is ALF compliant
df['exists'] = True  # Filename record exists on Alyx database

# Load sessions table so we can recuse the number of queries
from one.webclient import AlyxClient
alyx_client = AlyxClient()
files = alyx_client.download_cache_tables()
sessions = pd.read_parquet(next(x for x in files if 'sessions' in str(x)))

# Iterate over non-directory keys
for i, key in df['key'].iteritems():
    parsed = alfiles.full_path_parts(key, as_dict=True, assert_valid=False)
    df.at[i, 'isALF'] = parsed['object'] is not None
    dataset_name = PurePosixPath(alfio.remove_uuid_file(key, dry=True)).name
    dset = Dataset.objects.select_related('revision').filter(
        name=dataset_name, collection=parsed['collection'], revision__name=parsed['revision'])
    dset
